{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018/5/30\n",
    "# 本文没有 Matploylib 注解画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [\n",
    "              [1, 1, 'yes'],\n",
    "              [1, 1, 'yes'],\n",
    "              [1, 0, 'no'],\n",
    "              [0, 1, 'no'],\n",
    "              [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序清单3-1 计算给定数据集的香农熵\n",
    "from math import log\n",
    "def calcShannonEnt(dataSet):\n",
    "    # 数据集样本数\n",
    "    numEntries = len(dataSet)\n",
    "    # 定义字典，用于计数\n",
    "    labelCounts = {}\n",
    "    # 从数据集中，每次取出一行\n",
    "    for featVec in dataSet:\n",
    "        # 取出每一行的最后一列，即 'yes' or 'no'\n",
    "        currentLabel = featVec[-1]\n",
    "        # 判断 'yes' or 'no' 是否在字典中，不在加入计数为0，在则计数加1\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    # 定义容器，存放 熵\n",
    "    shannonEnt = 0.0\n",
    "    # 依据信息熵公式，计算该 dataSet 的信息熵 \n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "    # 返回 dataSet 的信息熵\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "['no surfacing', 'flippers']\n",
      "0.9709505944546686\n",
      "1.3709505944546687\n"
     ]
    }
   ],
   "source": [
    "# 测试一下\n",
    "myDat, labels = createDataSet()\n",
    "print(myDat)\n",
    "print(labels)\n",
    "print(calcShannonEnt(myDat))\n",
    "\n",
    "myDat[0][-1] = 'maybe'  # 熵越高，则混合的数据也越多\n",
    "print(calcShannonEnt(myDat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "a.append(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "a.extend(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#程序清单 3-2 按照给定特征划分数据集\n",
    "# dataSet 数据集\n",
    "# axis　　列号\n",
    "# value　将列号为axis，值为value的　其他数据分个出来，看实例\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis] \n",
    "            reducedFeatVec.extend(featVec[axis + 1 : ])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "[[1, 'no'], [1, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n",
      "[[1, 'no']]\n"
     ]
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "print(myDat)\n",
    "\n",
    "print(splitDataSet(myDat, 0, 1))\n",
    "print(splitDataSet(myDat, 0, 0))\n",
    "print(splitDataSet(myDat, 1, 1))\n",
    "print(splitDataSet(myDat, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1]\n",
      "[1 2]\n",
      "[1 2 3]\n",
      "\n",
      "\n",
      "[1 2 3]\n",
      "[2 3]\n",
      "[3]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "print(a[:0])\n",
    "print(a[:1])\n",
    "print(a[:2])\n",
    "print(a[:3])\n",
    "print('\\n')\n",
    "print(a[0:])\n",
    "print(a[1:])\n",
    "print(a[2:])\n",
    "print(a[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序清单 3-3 选择最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    # 每一行最后一列为 label,计算 feature 列数\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "#     print(dataSet[0])\n",
    "#     print(numFeatures)\n",
    "    #计算整个数据集的信息熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    # bestInfoGain　信息增益预先设为为 0.0\n",
    "    # bestFeature 最好划分的列标签，预先设为 -1\n",
    "    bestInfoGain = 0.0; bestFeature = -1\n",
    "    # 依据 feature 数进行循环\n",
    "    for i in np.arange(numFeatures):\n",
    "        # 将每一行数据取出，存为list，次数为feature数，即没有最后一列label\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # set 去冗余\n",
    "        uniqueVals = set(featList)\n",
    "        # 定义熵容器\n",
    "        newEntropy = 0.0\n",
    "        # 从 uniqueVals 集合中迭代\n",
    "        for value in uniqueVals:\n",
    "            # 从每一列开始，按值划分数据集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            # 见公式\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet) \n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        # 找到信息增益最大的列号\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    # 返回信息增益最大的列号\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\n",
      "[1, 1, 1, 0, 0]\n",
      "1 {0, 1}\n",
      "[1, 1, 0, 1, 1]\n",
      "1 {0, 1}\n",
      "[1, 1, 0, 1, 1]\n",
      "['yes', 'yes', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "print(chooseBestFeatureToSplit(myDat))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "for i in np.arange(2):\n",
    "    a = [b[i] for b in myDat]\n",
    "    print(a)\n",
    "    print('1', set(a))\n",
    "print(a)\n",
    "\n",
    "a = [b[-1] for b in myDat]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，\n",
    "# 在这种情况下，我们通常会采用 多数表决的方法决定该叶子节点的分类\n",
    "def majorityCnt(classList):\n",
    "    # 定义一个用于计数的字典\n",
    "    # 注，此时classList 只有一列，为类标签，因为类标签不唯一，才用此方法找最多的label\n",
    "    classCount = {}\n",
    "    # 从 classList 迭代取值\n",
    "    for vote in classList:\n",
    "        # 如果从classList中取出的值不在classCount字典中，则将该值放入字典，计数为1，否则在字典中的该值计数加1\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    \n",
    "    # 找到字典中 value 最大的 key 并返回\n",
    "    newvalue = -1\n",
    "    for key in classCount:\n",
    "        if newvalue < classCount[key]:\n",
    "            newkey = key\n",
    "            newvalue = classCount[key]\n",
    "    return newkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序清单 3-4 创建树的函数代码\n",
    "def createTree(dataSet, labels): # 两个输入参数-- 数据集， 标签列表\n",
    "    # 将 dataSet 最后一列放入 classList\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 如果类别完全相同则停止继续划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0] \n",
    "    \n",
    "    # 如果数据集已经处理了所有属性，但是类标签依然不是唯一的，采用 多数表决的方法决定该叶子节点的分类\n",
    "    if len(dataSet[0]) == 1:  \n",
    "        return majorityCnt(classList) \n",
    "    \n",
    "    # 得到最好划分，也就是信息增益最大的列号\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    # 将 信息增益最大的列的列名存入 bestFeatLabel\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    # 定义树，存为字典形式\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    # 将信息增益最大的列名删除\n",
    "    del(labels[bestFeat])\n",
    "    \n",
    "    # 将信息增益最大的列取出\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    # 去除冗余\n",
    "    uniqueVals = set(featValues)\n",
    "    # 迭代取值\n",
    "    for value in uniqueVals:\n",
    "        # 这行代码复制了类标签\n",
    "        subLabels = labels[:]  \n",
    "        # 递归创建树\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 字典的嵌套\n",
    "        #print('1  ',bestFeatLabel,' ',value,' ',myTree[bestFeatLabel][value])\n",
    "    #print('2 ',myTree, '\\n')\n",
    "    # 返回创建好的树\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "myTree = createTree(myDat, labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 1, 3, 5, 6, 1]\n",
      "3\n",
      "[1, 3, 4, 1, 3, 5, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 3, 4, 1, 3, 5, 6, 1]\n",
    "print(a)\n",
    "print(a.count(1))\n",
    "\n",
    "b = a[:]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no sur': {0: 'no'}}\n",
      "{'no sur': {0: 'no', 1: {'fli': {0: 'no', 1: 'yes'}}}}\n",
      "{'fli': {0: 'no', 1: 'yes'}}\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# 字典 嵌套\n",
    "s = {'no sur':{}}\n",
    "s['no sur'][0] = 'no'\n",
    "print(s)\n",
    "\n",
    "ss = {'fli':{}}\n",
    "ss['fli'][0] = 'no'\n",
    "ss['fli'][1] = 'yes'\n",
    "\n",
    "s['no sur'][1] = ss\n",
    "print(s)\n",
    "print(s['no sur'][1])\n",
    "print(s['no sur'][1]['fli'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用决策树执行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序清单3-8 使用决策树的分类函数\n",
    "# inputTree 创建好的决策树\n",
    "# featLabels 存放feature名的list\n",
    "# testVec   预测的feature\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    # 取出决策树的key，存为list，并取第一个key\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 取出第一个key所对应的value\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 取出 firstStr 所在的列号\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    # 这段代码为递归找到类别，依次递归向下找\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if isinstance(secondDict[key], dict):\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no surfacing', 'flippers']\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n",
      "no\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "print(labels)\n",
    "\n",
    "myTree = createTree(myDat, labels)\n",
    "print(myTree)\n",
    "\n",
    "# 经过 createTree 已经把labels给破坏了，所以现在要从新获取labels\n",
    "myDat, labels = createDataSet()\n",
    "print(classify(myTree, labels, [1, 0]))\n",
    "print(classify(myTree, labels, [1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [123, 'aa', 'bb', 'vv']\n",
    "print(a.index('aa'))\n",
    "print(a.index('vv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序清单 3-9 使用pickle模块存储决策树\n",
    "import pickle\n",
    "def storeTree(inputTree, filename):\n",
    "    fw = open(filename, 'wb')\n",
    "    pickle.dump(inputTree, fw)\n",
    "    fw.close()\n",
    "    \n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no surfacing', 'flippers']\n",
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "myDat, labels = createDataSet()\n",
    "print(labels)\n",
    "myTree = createTree(myDat, labels)\n",
    "\n",
    "storeTree(myTree, '/home/gcb/data/a.txt')\n",
    "\n",
    "print(grabTree('/home/gcb/data/a.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用决策树预测隐形眼镜类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tearRate': {'normal': {'astigmatic': {'yes': {'prescript': {'myope': 'hard', 'hyper': {'age': {'presbyopic': 'no lenses', 'young': 'hard', 'pre': 'no lenses'}}}}, 'no': {'age': {'presbyopic': {'prescript': {'myope': 'no lenses', 'hyper': 'soft'}}, 'young': 'soft', 'pre': 'soft'}}}}, 'reduced': 'no lenses'}}\n"
     ]
    }
   ],
   "source": [
    "fr = open('/home/gcb/data/lenses.txt')\n",
    "lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n",
    "# print(lenses)\n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n",
    "lensesTree = createTree(lenses, lensesLabels)\n",
    "print(lensesTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
